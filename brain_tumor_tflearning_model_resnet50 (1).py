# -*- coding: utf-8 -*-
"""Brain_Tumor_TFLearning_model_resnet50.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qA5z8s1_ejYEz8SEUMv6ajI3CN0bralj
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import zipfile
import pickle
import h5py
import cv2
from google.colab import drive
from google.colab.patches import cv2_imshow
from sklearn.metrics import confusion_matrix
import seaborn as sns
# %matplotlib inline

# Commented out IPython magic to ensure Python compatibility.
import scipy as sp
import scipy.ndimage as spi
import numpy as np
import pandas as pd
import PIL

import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

import matplotlib.pyplot as plt

# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

"""1 for meningioma, 2 for glioma, 3 for pituitary tumor"""

image_size = (256, 256)
BATCH_SIZE = 32

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/Colab Notebooks/dataset/BrainTumor",
    validation_split=0.2,
    subset="training",
    seed=1337,
    image_size=image_size,
    batch_size= BATCH_SIZE,
)
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/Colab Notebooks/dataset/BrainTumor",
    validation_split=0.2,
    subset="validation",
    seed=1337,
    image_size=image_size,
    batch_size=BATCH_SIZE,
)

class_names = [ 'meningioma', 'glioma','pituitary']
plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(9):
      ax = plt.subplot(3, 3, i + 1)
      plt.imshow(images[i].numpy().astype("uint8"))
      plt.title(class_names[int(labels[i])])
      plt.axis("off")

data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal_and_vertical"),
        layers.RandomRotation(0.2),
    ]
)

plt.figure(figsize=(10, 10))
for images, _ in train_ds.take(1):
    for i in range(9):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(augmented_images[0].numpy().astype("uint8"))
        plt.axis("off")

augmented_train_ds = train_ds.map(
  lambda x, y: (data_augmentation(x, training=True), y))

val_batches = tf.data.experimental.cardinality(val_ds)
test_ds = val_ds.take(val_batches // 5)
val_ds = val_ds.skip(val_batches // 5)

train_ds = augmented_train_ds.prefetch(buffer_size=32)
val_ds = val_ds.prefetch(buffer_size=32)
test_ds = test_ds.prefetch(buffer_size=32)

image_shape = image_size + (3,)
base_model = tf.keras.applications.ResNet50(input_shape=image_shape,
                                               include_top=False,
                                               weights='imagenet')

base_model.trainable = False

image_batch, label_batch = next(iter(train_ds))
feature_batch = base_model(image_batch)
print(feature_batch.shape)

base_model.summary()

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
feature_batch_average = global_average_layer(feature_batch)
print(feature_batch_average.shape)

prediction_layer = tf.keras.layers.Dense(3)
prediction_batch = prediction_layer(feature_batch_average)
print(prediction_batch.shape)

preprocess_input = tf.keras.applications.resnet50.preprocess_input

inputs = tf.keras.Input(shape=(256, 256, 3))
x = data_augmentation(inputs)
x = preprocess_input(inputs)
x = base_model(x, training=False)
x = global_average_layer(x)
x = tf.keras.layers.Dropout(0.4)(x)
x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(2048,activation='relu')(x)
x = tf.keras.activations.sigmoid(x)
x = tf.keras.layers.Dropout(0.4)(x)
x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(2048,activation='relu')(x)
x = tf.keras.activations.sigmoid(x)

outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

SVG(model_to_dot(model,
                 show_shapes= True,
                 show_layer_names=True,
                 dpi=65).create(prog='dot', format='svg'))

model.summary()

BASE_LEARNING_RATE = 0.0001
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = BASE_LEARNING_RATE),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics=['accuracy'])

len(model.trainable_variables)

loss0, accuracy0 = model.evaluate(val_ds)
print("initial loss: {:.2f}".format(loss0))
print("initial accuracy: {:.2f}".format(accuracy0))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = model.fit(train_ds,
#                          epochs=30,
#                          validation_data=val_ds,
#                          shuffle=True
#                         )

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
EPOCHS = 30
epochs_range = range(0, EPOCHS)
fig, axes = plt.subplots(1, 2, figsize=(10, 5))
ax1 = axes[0]
ax2 = axes[1]
ax1 = plt.subplot(1,2,1)
      
ax1.set_xlabel("Epochs")
ax1.set_ylabel("Accuracy")
ax1.set_title("Accuracy Metrics")
ax1.plot(epochs_range, acc)
ax1.plot(epochs_range, val_acc)
ax1.legend(["Accuracy", "Validation Accuracy"], loc ="upper left")
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(0, EPOCHS)

ax2.set_xlabel("Epochs")
ax2.set_ylabel("Loss")
ax2.set_title("Loss Metrics")
ax2.plot(epochs_range, loss)
ax2.plot(epochs_range, val_loss)
ax2.legend(["Loss", "Validation Loss"], loc ="upper left")

fig.tight_layout()

model.summary()

img = keras.preprocessing.image.load_img(
    "/content/drive/MyDrive/Colab Notebooks/dataset/BrainTumor/meningioma/114.jpg",
     target_size=image_size
)
img2 = keras.preprocessing.image.load_img(
    "/content/drive/MyDrive/Colab Notebooks/dataset/BrainTumor/glioma/1850.jpg",
     target_size=image_size
)
img3 = keras.preprocessing.image.load_img(
    "/content/drive/MyDrive/Colab Notebooks/dataset/yes/y1000.jpg",
    target_size=image_size
)
img4 = keras.preprocessing.image.load_img(
    "/content/drive/MyDrive/Colab Notebooks/dataset/yes/y1006.jpg",
    target_size=image_size
)
img5 = keras.preprocessing.image.load_img(
    "/content/drive/MyDrive/Colab Notebooks/dataset/yes/y548.jpg",
    target_size=image_size
)
img6 = keras.preprocessing.image.load_img(
      "/content/drive/MyDrive/Colab Notebooks/dataset/yes/y60.jpg",
      target_size=image_size
)
img7 = keras.preprocessing.image.load_img(
      "/content/drive/MyDrive/Colab Notebooks/dataset/yes/y148.jpg", 
      target_size=image_size
)
image_array = [img, img2, img3, img4, img5, img6,img7]
for image in image_array :

    img_array = keras.preprocessing.image.img_to_array(image)
    # Create batch axis
    img_array = tf.expand_dims(img_array, 0)  
    predictions = model.predict(img_array)

    score = tf.nn.softmax(predictions[0])

    print(
            "This image most likely belongs to {} with a {:.2f} percent confidence."
            .format(class_names[np.argmax(score)], 100 * np.max(score))
        )

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Colab Notebooks/
model.save('model_final_multi-classification.h5')

loss, accuracy = model.evaluate(test_ds)
print('Test accuracy :', accuracy)

labels_entire = []
pred_entire = []
for image_batch,label_batch in test_ds.as_numpy_iterator():
    prediction = model.predict_on_batch(image_batch).flatten()

    # Apply a sigmoid since our model returns logits
    predictions = tf.nn.sigmoid(prediction).numpy()

    n = 0
    predict = []
    while n<=(predictions.shape[0]-3):
        pred = np.argmax(predictions[n:n+3]) #Returns the index of the largest element in the selected subarray
        n+=3
        pred_entire.append(pred)
    for el in label_batch:
        labels_entire.append(el)
pred_entire = np.array(pred_entire)
labels_entire = np.array(labels_entire)
print(pred_entire)
print(labels_entire)

arr = confusion_matrix(labels_entire, pred_entire)
df_cm = pd.DataFrame(arr, class_names, class_names)
plt.figure(figsize = (9,6))
sns.heatmap(df_cm, annot=True, fmt="d", cmap='viridis')
plt.xlabel("Prediction")
plt.ylabel("Target")
plt.show()

from sklearn import metrics
fpr, tpr, thresholds = metrics.roc_curve(labels_entire, pred_entire, pos_label=2)
metrics.auc(fpr, tpr)